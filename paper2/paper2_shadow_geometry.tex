\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\doublespacing

\title{The Geometry of Biological Shadows: Quantifying Topological Aliasing in High-Dimensional Systems}

\author{Ian Todd\\
Sydney Medical School\\
University of Sydney\\
Sydney, NSW, Australia\\
\texttt{itod2305@uni.sydney.edu.au}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In Todd (2025), we argued that Popperian falsifiability saturates in high-dimensional biological systems. Here, we provide computational tools to \emph{measure} this loss. We distinguish \textbf{system dimensionality} ($D_{\text{sys}}$)---intrinsic degrees of freedom---from \textbf{observation dimensionality} ($D_{\text{obs}}$)---coordinates accessible to measurement. Using the Lorenz attractor, we demonstrate that when $D_{\text{sys}} > D_{\text{obs}}$, binary classification produces \textbf{topological aliasing}: \textbf{47\%} of states are misclassified, with apparent ``teleportations'' in the shadow that do not exist in the underlying system. We validate this in real data: across four scRNA-seq datasets (n = 90,300 cells), \textbf{75.5\% of apparent neighbors in t-SNE projections were not neighbors in high-dimensional space}. Meanwhile, state-space coverage collapses to \textbf{$<$0.01\%}. This is geometry, not noise---the consequence of projecting $D_{\text{sys}} \approx 10$--40 into $D_{\text{obs}} = 2$. We quantify $\sqrt{N}$ scaling of sub-Landauer signal detection via stochastic resonance, and map the \textbf{falsifiability regime diagram}---the boundary where binary hypothesis testing becomes incoherent. We formalize an \textbf{inference trilemma}: non-ergodicity precludes time averaging, dimensionality precludes ensemble averaging, and thermodynamics precludes direct measurement---blocking all classical escape routes. All analyses use the open-source \texttt{falsifiability} toolkit.
\end{abstract}

\textbf{Keywords:} falsifiability, topological aliasing, dimensionality, single-cell RNA sequencing, t-SNE, UMAP, biological epistemology

\section{Introduction}

In a recent paper \cite{todd2025limits}, we argued that Karl Popper's falsifiability criterion---the requirement that scientific hypotheses be reducible to binary tests---faces fundamental limits in high-dimensional biological systems. That argument was primarily epistemological, drawing on the conjunction of high dimensionality, thermodynamic measurement bounds (the Landauer limit), and chaotic dynamics to show that binary projection destroys causal structure.

However, the earlier work remained qualitative. It established \emph{that} falsifiability saturates but did not provide tools to measure \emph{where} or \emph{how much}. The present paper addresses this gap by developing computational methods to quantify the loss of information under dimensional projection.

\subsection{The Core Problem: System vs Shadow}

The central insight is \textbf{ontological}, not merely statistical. We distinguish between:

\begin{table}[h]
\centering
\begin{tabular}{@{}lp{7cm}p{4cm}@{}}
\toprule
\textbf{Concept} & \textbf{Definition} & \textbf{Example} \\
\midrule
$D_{\text{sys}}$ & The intrinsic degrees of freedom of the dynamical system---the dimension of the manifold the system actually occupies in phase space (e.g., correlation dimension, participation ratio) & Neural attractor dimension \\
\addlinespace
$D_{\text{obs}}$ & The number of coordinates we record---sensors, channels, features & Number of electrodes \\
\bottomrule
\end{tabular}
\caption{The ontological distinction underlying limits to falsifiability.}
\label{tab:dimensions}
\end{table}

Standard falsifiability assumes we can project from system to observation via a binary cut without destroying causal structure. The simulations presented here demonstrate that when $D_{\text{sys}} > D_{\text{obs}}$, this assumption fails systematically: distinct causal states \emph{alias} to the same observation, and apparent discontinuities appear in the shadow that do not exist in the underlying system.

We call this phenomenon \textbf{topological aliasing}---the observation space contains ``jumps'' that are artifacts of projection, not features of reality.

\subsection{Contributions}

This paper makes five contributions:

\begin{enumerate}
    \item \textbf{Topological Aliasing Metric}: Using the Lorenz attractor as a minimal model, we quantify the rate at which dynamical states are misclassified under projection and the number of false discontinuities introduced.

    \item \textbf{Empirical Validation on scRNA-seq}: We demonstrate that 75.5\% of apparent neighbors in t-SNE projections of standard single-cell datasets are aliased---wrong about the underlying high-dimensional relationships. This is not a property of any particular dataset but a geometric consequence of the dimension gap.

    \item \textbf{Stochastic Resonance Scaling}: We verify the $\text{SNR} \propto \sqrt{N}$ scaling predicted in \cite{todd2025limits}, showing that sub-Landauer patterns become detectable only at the ensemble level.

    \item \textbf{Falsifiability Regime Diagram}: We map the boundary in (dimensionality $\times$ signal strength) space where binary hypothesis testing transitions from effective to incoherent.

    \item \textbf{The Inference Trilemma}: We formalize the structural blocks preventing classical inference---non-ergodicity (blocking time averaging), dimensional explosion (blocking ensemble averaging), and thermodynamic fragility (blocking direct measurement)---and provide simulations demonstrating each failure mode (\S4.2).
\end{enumerate}

\section{Methods}

All simulations were implemented in Python using NumPy, SciPy, and Matplotlib. Code is available at \url{https://github.com/todd866/limits-of-falsifiability}.

\subsection{The Shadow Box: Lorenz Attractor Projection}

To demonstrate topological aliasing in a controlled setting, we use the Lorenz system \cite{lorenz1963}:
\begin{align}
\dot{x} &= \sigma(y - x) \\
\dot{y} &= x(\rho - z) - y \\
\dot{z} &= xy - \beta z
\end{align}
with standard parameters $\sigma = 10$, $\rho = 28$, $\beta = 8/3$. This system has fractal dimension $D_{\text{sys}} \approx 2.06$ and exhibits two distinct dynamical lobes (left: $x < 0$, right: $x > 0$) connected by smooth transitions. Because the attractor has fractal dimension $>2$, any 2D projection is necessarily non-injective: multiple distinct points in the 3D attractor map to the same $(y, z)$ coordinates. The resulting overlaps are the geometric origin of the aliasing we quantify here.

We integrate for $T = 100$ time units using Euler's method ($dt = 0.01$) and subsample every 10 steps, yielding 1000 points on the attractor.

\textbf{The shadow}: We observe only the $(y, z)$ projection---the $x$ coordinate is hidden. This models the common biological situation where a key variable is sub-threshold, unmeasured, or integrated out.

\textbf{The binary cut}: We apply a classifier $\hat{L} = \mathbf{1}[z > 25]$ in the shadow space and compare to the true lobe label $L = \mathbf{1}[x > 0]$.

\textbf{Metrics}:
\begin{itemize}
    \item \textbf{Aliasing rate}: Fraction of points where $\hat{L} \neq L$
    \item \textbf{Topological violations}: Number of times $\hat{L}$ changes sign without a corresponding change in $L$---apparent ``teleportations'' in the shadow
    \item \textbf{Type II error rate}: False negative rate for each lobe
\end{itemize}

These metrics extend to empirical trajectories: given any proposed binary partition in observation space and any ground-truth labels on the underlying system (e.g., lobe labels from a full model, or high-dimensional clustering), one can compute aliasing and topological violation rates to assess how faithfully the observation partitions track the system's causal structure.

\subsection{Sub-Landauer Signal Detection}

To model the sub-Landauer regime, we simulate a population of $N$ threshold neurons receiving a common weak periodic signal:
\begin{equation}
s(t) = A \sin(2\pi f t), \quad A = 0.3, \quad f = 2 \text{ Hz}
\end{equation}
with detection threshold $\theta = 1.0$ (so $A < \theta$: individually undetectable). Each neuron receives independent Gaussian noise with $\sigma_{\text{noise}} = 0.8$.

The population average $Y(t) = \frac{1}{N}\sum_{i=1}^N y_i(t)$ is computed, and we measure the signal-to-noise ratio via spectral analysis. We test population sizes $N \in \{1, 5, 10, 25, 50, 100, 200, 500\}$.

\subsection{Scale-Dependent Falsifiability}

To map the regime boundary, we generate pairs of high-dimensional Gaussian distributions:
\begin{itemize}
    \item Model A: Isotropic Gaussian $\mathcal{N}(0, I_n)$
    \item Model B: Gaussian with structure along a random direction, controlled by signal strength $s$
\end{itemize}

For each (dimension $n$, signal strength $s$) pair, we compute:
\begin{itemize}
    \item \textbf{Binary test power}: Best single-coordinate $t$-test
    \item \textbf{Multivariate test power}: Hotelling's $T^2$ statistic
\end{itemize}
Power is estimated as the fraction of 50 trials where the null hypothesis is rejected at $\alpha = 0.05$.

\subsection{Non-Ergodic Memory}

To demonstrate why time averaging fails when hidden dimensions carry memory, we compare two dynamical systems:

\textbf{Ergodic system}: A mean-reverting random walk $x_{t+1} = 0.98 x_t + 0.02 \cdot 0.5 + \epsilon_t$ where $\epsilon_t \sim \mathcal{N}(0, 0.03^2)$. All trajectories explore the same region of state space, and time averages converge to the ensemble mean regardless of initial condition.

\textbf{Non-ergodic system}: The same dynamics, but with a \emph{hidden state} $H \in \{0, 1\}$ set at initialization that determines which attractor the visible state is drawn toward: $x_{t+1} = 0.98 x_t + 0.02 \cdot a_H + \epsilon_t$, where $a_0 = 0.25$ and $a_1 = 0.75$.

An observer seeing only $x$ cannot determine $H$ directly. Their time average converges---but to which value depends on the hidden state they cannot observe. The ensemble mean (0.5) is achieved by \emph{no individual trajectory}.

\subsection{Topological Aliasing in Single-Cell RNA Sequencing}

To validate aliasing in real biological data, we analyzed four standard scRNA-seq datasets spanning different species, tissues, and sample sizes:

\begin{table}[h]
\centering
\begin{tabular}{@{}llrr@{}}
\toprule
\textbf{Dataset} & \textbf{Source} & \textbf{Cells} & \textbf{Genes} \\
\midrule
Sade-Feldman (Melanoma) & Human tumor-infiltrating lymphocytes & 16,291 & 16,906 \\
PBMC 3k & Human peripheral blood (10X Genomics) & 2,700 & 1,838 \\
Paul15 (Bone Marrow) & Mouse hematopoiesis & 2,730 & 3,451 \\
PBMC 68k & Human peripheral blood (10X Genomics) & 68,579 & 11,267 \\
\bottomrule
\end{tabular}
\caption{Single-cell datasets used for aliasing validation.}
\label{tab:datasets}
\end{table}

For each dataset, we computed:
\begin{enumerate}
    \item \textbf{Intrinsic dimensionality} ($D_{\text{sys}}$) via participation ratio on PCA-reduced data (50 components)
    \item \textbf{t-SNE embedding} using standard parameters (perplexity = 30)
    \item \textbf{Topological aliasing rate}: For each point, we computed the $k = 15$ nearest neighbors in both the high-dimensional PCA space and the 2D t-SNE space. Aliasing is defined as $1 - \text{mean}(\text{Jaccard similarity})$, where the Jaccard similarity measures the overlap between the two neighbor sets.
\end{enumerate}

This metric captures how much the 2D projection ``lies'' about neighborhood relationships---the fundamental structure that clustering algorithms rely upon.

\subsection{Sample Complexity and Coverage}

To demonstrate why ensemble averaging becomes impractical in high dimensions, we measure the \emph{coverage} of a $n$-dimensional state space by $N$ samples.

We discretize each dimension into $k = 3$ bins, giving $3^n$ total cells. Coverage is the fraction of cells containing at least one sample. This is deliberately generous---coarser binning than most biological applications would require.

We test dimensions $n \in \{2, 3, \ldots, 15\}$ with fixed sample size $N = 1000$. The curse of dimensionality predicts coverage $\to 0$ as $n$ increases, since $3^n$ grows exponentially while $N$ remains constant.

\section{Results}

\subsection{Topological Aliasing in the Shadow Box}

Figure~\ref{fig:shadow_box} shows the Lorenz attractor and its 2D projection.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_shadow_box.pdf}
\caption{\textbf{The Shadow Box---Topological aliasing when $D_{\text{sys}} > D_{\text{obs}}$.} (A) Full 3D Lorenz attractor ($D_{\text{sys}} = 3$) colored by true dynamical state. (B) 2D projection ($D_{\text{obs}} = 2$) with binary cut at $z = 25$. (C) Aliasing revealed: 47\% of points misclassified, with 199 apparent ``teleportations'' where the shadow jumps across the cut while the system flows continuously. See \texttt{toolkit/demo\_lorenz.py}.}
\label{fig:shadow_box}
\end{figure}

Key results:
\begin{itemize}
    \item \textbf{Aliasing rate}: 47\% of states are misclassified by the shadow cut
    \item \textbf{Classification accuracy}: 53\% (barely above chance)
    \item \textbf{Topological violations}: 199 false discontinuities---times the shadow ``teleports'' while the system flows continuously
    \item \textbf{Type II error}: 47\% for both lobes
\end{itemize}

The binary cut that appears ``clean'' in the shadow is fundamentally wrong about the underlying dynamics. Nearly half of all states are aliased---placed in the wrong causal category by the projection.

\subsection{Topological Aliasing in Single-Cell Data}

The Lorenz demonstration establishes aliasing in a minimal model. But does this phenomenon occur in real biological data? Figure~\ref{fig:scrna} presents results from four standard scRNA-seq datasets.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_multi_dataset.pdf}
\caption{\textbf{Multi-dataset evidence---High-dimensional systems systematically alias under projection.} (A) Schematic ``hairball of truth'': dark red lines connect false neighbors (2D neighbors that were not neighbors in high-D). (B) Aliasing rates across four scRNA-seq benchmarks, sorted by magnitude; all exceed random expectation (50\%), mean = 75.5\%. (C) $D_{\text{sys}}$ (participation ratio) vs aliasing. (D) Summary. See \texttt{toolkit/demo\_scrna.py}.}
\label{fig:scrna}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Dataset} & \textbf{$D_{\text{sys}}$} & \textbf{Aliasing} & \textbf{Coverage} \\
\midrule
Sade-Feldman (Melanoma) & 12.5 & 66.2\% & 0.0003\% \\
PBMC 3k (10X) & 14.8 & 83.1\% & 0.0001\% \\
Paul15 (Bone Marrow) & 8.7 & 78.4\% & 0.002\% \\
PBMC 68k (10X) & 38.7 & 74.3\% & $<$0.0001\% \\
\midrule
\textbf{Average} & \textbf{18.7} & \textbf{75.5\%} & --- \\
\bottomrule
\end{tabular}
\caption{Topological aliasing rates across standard scRNA-seq datasets. Coverage computed with $k=3$ bins per dimension.}
\label{tab:results}
\end{table}

Key findings:
\begin{itemize}
    \item \textbf{Aliasing is universal}: All four datasets show $>65\%$ aliasing, regardless of species (human vs mouse), tissue (blood vs tumor vs bone marrow), or sample size (2,700 to 68,579 cells).
    \item \textbf{Aliasing correlates with dimensionality}: Higher $D_{\text{sys}}$ generally predicts higher aliasing, as expected from the geometric argument.
    \item \textbf{The ``hairball'' is dense}: The visual density of false-neighbor connections (Figure~\ref{fig:scrna}B) demonstrates that this is not an edge effect---aliasing pervades the entire embedding.
\end{itemize}

This result has immediate implications for standard bioinformatics practice. When researchers draw cluster boundaries on t-SNE/UMAP plots, approximately \textbf{three-quarters of the neighborhood relationships those boundaries rely upon are wrong}. The clusters may be real, but the boundaries are not where they appear.

\subsection{Stochastic Resonance Scaling}

Figure~\ref{fig:sr} shows the signal-to-noise ratio as a function of population size.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/fig_sub_landauer_sr.pdf}
\caption{\textbf{Sub-Landauer signal detection via stochastic resonance.} (A) The sub-threshold signal (amplitude 0.3, threshold 1.0). (B) Single neuron vs population average response. (C) SNR scales as $\sqrt{N}$ (dashed line = theory).}
\label{fig:sr}
\end{figure}

The signal is individually undetectable ($A/\theta = 30\%$), but the population average reveals it clearly. The scaling $\text{SNR} \propto \sqrt{N}$ is confirmed:
\begin{itemize}
    \item At $N = 100$: SNR $\approx 30$
    \item At $N = 500$: SNR $\approx 135$
    \item Ratio: $135/30 \approx 4.5 \approx \sqrt{5}$
\end{itemize}

This demonstrates that \textbf{falsifiability is an emergent property of the ensemble}, not the individual unit. Sub-Landauer patterns can only be accessed through collective measurement.

\subsection{The Falsifiability Regime Diagram}

Figure~\ref{fig:regime} maps the boundary between effective and incoherent binary testing.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_scale_dependent.pdf}
\caption{\textbf{Scale-dependent falsifiability.} (A) Binary test power across dimension $\times$ signal strength; red indicates low power. (B) Multivariate test power; maintains efficacy at higher dimensions. (C) The falsifiability regime diagram: ``Popper Regime'' (low dimensionality, high signal) where binary tests work vs ``Ensemble Regime'' (high dimensionality, low signal) where only multivariate methods succeed. Arrow indicates the transition as systems become more complex.}
\label{fig:regime}
\end{figure}

Key findings:
\begin{itemize}
    \item At $n = 50$ dimensions, binary tests never achieve 80\% power even at maximum signal strength
    \item Multivariate methods maintain power across the full range
    \item The ``Popper regime'' (where binary tests work) occupies only the low-$n$, high-signal corner
    \item The ``Ensemble regime'' (where only multivariate methods work) dominates as dimensionality increases
\end{itemize}

\subsection{Non-Ergodic Time Averages}

Figure~\ref{fig:nonergodic} demonstrates why time averaging fails when hidden dimensions carry memory.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_nonergodic_memory.pdf}
\caption{\textbf{Non-ergodic memory---Time averaging fails when hidden states store history.} (A--B) Raw trajectories: ergodic systems explore the same region; non-ergodic systems diverge by hidden state $H$. (C--D) Cumulative time averages: ergodic converge to ensemble mean; non-ergodic converge to $H$-dependent values. The ensemble mean (0.5) is reached by no individual trajectory. (E) Final distribution: ergodic = tight; non-ergodic = bimodal. (F) Schematic. See \texttt{toolkit/demo\_memory.py}.}
\label{fig:nonergodic}
\end{figure}

Key results:
\begin{itemize}
    \item Trajectories with $H = 0$ converge to time average $\approx 0.25$
    \item Trajectories with $H = 1$ converge to time average $\approx 0.75$
    \item The ensemble mean (0.5) is achieved by \emph{no individual trajectory}
    \item A $t$-test between groups yields $p \ll 0.001$---this is not noise, but structural bias
\end{itemize}

This is why ``just measure longer'' does not help when the system remembers in dimensions you cannot see.

\subsection{Coverage Collapse in High Dimensions}

Figure~\ref{fig:coverage} demonstrates why ensemble averaging becomes impractical as dimensionality increases.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_sample_complexity.pdf}
\caption{\textbf{The curse of dimensionality for coverage.} (A) Coverage fraction collapses exponentially with dimension at fixed $N = 1000$ samples; dashed line marks 1\% coverage. (B) State space size explodes as $3^n$; shaded region shows where space exceeds sample size. (C) Coverage curves by dimension: even $10^4$ samples achieve negligible coverage at $n = 10$.}
\label{fig:coverage}
\end{figure}

Key results (with $k = 3$ bins per dimension, $N = 1000$ samples):
\begin{itemize}
    \item At $n = 5$: 99\% coverage (adequate)
    \item At $n = 10$: 1.7\% coverage (sparse)
    \item At $n = 15$: 0.007\% coverage (effectively zero)
\end{itemize}

The curse is \emph{exponential}: doubling sample size buys almost nothing in coverage. One would need to exponentiate the sample size to keep pace with dimensional growth---functionally impossible for biological systems with $n > 10$.

\section{Discussion}

\subsection{Topological Aliasing as a Fundamental Limit}

The shadow box demonstration reveals that the failure of binary falsification in high-dimensional systems is not merely a matter of ``insufficient data'' or ``noisy measurements.'' It is a \textbf{geometric fact}: when the observation space has lower dimension than the system, distinct causal states necessarily map to the same observation.

The 199 topological violations are particularly striking. A falsificationist observing only the shadow would see the system ``teleport'' across the binary cut hundreds of times---apparent discontinuities that invite causal explanations (``something switched!'') but are entirely artifacts of projection.

This has direct implications for biology. When a cell appears to ``decide'' between fates, when a neural circuit appears to ``switch'' between states, when an ecosystem appears to ``flip'' between regimes---these discontinuities in the observation may not exist in the underlying high-dimensional dynamics. The system may be flowing smoothly through dimensions we cannot see.

This work provides a computational operationalization of the ``internal measurement'' problem discussed extensively in this journal \cite{igamberdiev2021, louie2020}. While prior work established the epistemological limits of reductionism through relational biology and the modeling relation, the \texttt{falsifiability} toolkit allows us to \textit{quantify} the measurement gap in specific datasets. We show that the ``computational irreducibility'' described by Azadi \cite{azadi2025} manifests empirically as a topological aliasing rate exceeding 75\% in standard transcriptomic data. The aliasing rate is, in effect, a measure of how much causal structure is lost when we attempt to reduce the system ($D_{\text{sys}} \to D_{\text{obs}}$).

\subsection{The Ensemble as the Appropriate Observable}

The stochastic resonance results confirm that sub-Landauer patterns---structures with energy below $k_B T \ln 2$---become accessible only through population averaging. This is not a technological limitation but a thermodynamic one: extracting a bit of information requires at least $k_B T \ln 2$ of energy, so patterns below this threshold cannot be resolved individually.

The $\sqrt{N}$ scaling provides a quantitative answer to ``how much pooling is needed?'' To detect a signal at fraction $f$ of threshold, one needs $N \gtrsim (1/f)^2$ units.

Crucially, sub-Landauer structure is not exotic---it is where most physical information resides. An atom's electron orbitals, a protein's tertiary geometry, a cell's spatial organization all represent structure encoded below the Landauer limit. This structure \emph{exists} independent of measurement; energy injection merely \emph{accesses} it. In atomic physics, high-energy probes (spectroscopy, scattering) can read out sub-Landauer structure without fundamentally disrupting the system. In biology, this is often impossible: the energy required to resolve fine-grained causal structure perturbs the very phenomenon under study. A protein's conformational landscape is information waiting to be read by ligand binding---but we cannot inject arbitrary energy to ``see it better'' without denaturing the protein. This asymmetry explains why the epistemological problem is more severe in living systems: biology must infer latent structure through statistics, not direct measurement.

There is a further consequence. Sub-Landauer structural changes---a phosphorylation, a methylation, a subtle conformational shift---can encode \emph{memory}. The system's past is written into its geometry. This breaks ergodicity: the assumption that time averages equal ensemble averages, which underwrites much of statistical mechanics. If unmeasured (but not unmeasurable) structure carries history, then samples drawn at different times are not independent draws from a stationary distribution---they are path-dependent snapshots of a trajectory we cannot fully observe. Ensemble averaging still works, but it reveals statistics of the trajectory, not the trajectory itself. The system ``remembers'' in dimensions we cannot see.

This creates a trilemma for high-dimensional biological inference:
\begin{enumerate}
    \item \textbf{Time averaging fails} (Figure~\ref{fig:nonergodic}): non-ergodicity means temporal samples are not equivalent to ensemble samples.
    \item \textbf{Ensemble averaging is impractical} (Figure~\ref{fig:coverage}): the curse of dimensionality demands sample sizes that scale as $k^n$---functionally infinite for biological dimensions.
    \item \textbf{Direct measurement is destructive} (Figure~\ref{fig:sr}): energy injection sufficient to resolve sub-Landauer structure perturbs the system beyond recovery.
\end{enumerate}
All three classical escape routes from measurement uncertainty are blocked. Even a correctly specified Bayesian model cannot rescue information destroyed by projection; posterior concentration in high dimensions suffers from the same measure-concentration pathologies \cite{gorban2018}, meaning the aliasing rate places a hard upper bound on any inference---frequentist or Bayesian. This is not a technological limitation awaiting better instruments; it is a structural feature of high-dimensional systems operating near thermodynamic limits.

\subsection{When Does Popper Apply?}

The regime diagram provides a practical answer: binary falsification works when dimensionality is low ($n \lesssim 10$) and signal strength is high. This covers much of classical biology---Mendelian genetics, enzyme kinetics, action potential propagation.

But as we move to systems biology, neuroscience, ecology, and evolution---domains with $n \gg 10$ and signals often near threshold---we enter the ensemble regime where Popperian logic becomes ``geometrically incoherent'' (to use the phrase from \cite{todd2025limits}).

The exact threshold ($\sim$10--20 dimensions) is model-dependent: the specific geometry of the signal structure, the choice of binary partition, and the sample size all affect where the transition occurs. However, the qualitative shape of the regime diagram---a small ``Popper corner'' at low dimensions and high signal, with ensemble methods dominating elsewhere---appears robust across the parameter space we explored.

This is not a failure of science but a recognition of scale-dependent epistemology. Different regimes require different methods.

\subsection{Recommendations for Practitioners}

We do not argue that dimension reduction should be abandoned. t-SNE, UMAP, and related methods remain valuable for visualization and exploratory analysis. Rather, we propose that the \textbf{Topological Aliasing Rate} be reported as a standard quality metric alongside 2D visualizations, analogous to how sequencing depth or mapping quality are reported in genomics.

Practical guidelines:
\begin{itemize}
    \item \textbf{Aliasing $<$ 30\%}: Visual clusters likely reflect high-dimensional topology. Binary hypotheses about cluster membership may be testable in the projection.
    \item \textbf{Aliasing 30--60\%}: Clusters should be interpreted with caution. Hypothesis testing should be performed in the original high-dimensional space (e.g., PCA coordinates) rather than the 2D embedding.
    \item \textbf{Aliasing $>$ 60\%}: The 2D projection is a ``visual convenience'' only. Cluster boundaries are more wrong than right. All quantitative analysis must occur in high-dimensional space.
\end{itemize}

The \texttt{falsifiability} toolkit (see Code Availability) computes aliasing rates for any high-dimensional dataset with a single function call, enabling researchers to calibrate their interpretive confidence before drawing biological conclusions.

\section{Conclusion}

We have provided computational tools to measure the limits of falsifiability that were theoretically derived in \cite{todd2025limits}. The key results are:

\begin{enumerate}
    \item \textbf{Topological aliasing is quantifiable}: In minimal models, nearly half of dynamical states are misclassified under projection, and hundreds of false discontinuities appear.

    \item \textbf{Sub-Landauer detection scales as $\sqrt{N}$}: Ensemble methods are not optional but thermodynamically required for weak signals.

    \item \textbf{The falsifiability boundary is mappable}: In our Gaussian benchmarks, binary tests fail systematically above $\sim$10--20 dimensions for moderate effect sizes, while multivariate methods continue to perform well.

    \item \textbf{The inference trilemma is structural}: Non-ergodicity, dimensional explosion, and thermodynamic fragility block the three classical paths to inference (time averaging, ensemble averaging, direct measurement), necessitating a shift to indirect, ensemble-based validation.
\end{enumerate}

These results operationalize an epistemological argument into measurable quantities. Biologists studying high-dimensional systems can now ask: ``What is the aliasing rate of my observation?'' and ``Am I in the Popper regime or the ensemble regime?''

The answer determines which inferential tools are appropriate.

\section*{Acknowledgements}
The author thanks Tara Eicher for early encouragement that led to pursuing publication of the first paper in this series, and the Fulcher Lab at the University of Sydney for critical feedback on dimensionality definitions that motivated this computational follow-up.

\section*{Funding}
This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

\section*{Declaration of competing interest}
The author declares no competing interests.

\section*{Declaration of generative AI}
This paper was developed using Claude 4.5 Opus (Anthropic), GPT-5.1 Pro (OpenAI), and Gemini 3 Pro (Google) for editing, code development, and refining arguments. The author reviewed and edited all content and takes full responsibility for the final article.

\section*{Code availability}
All simulation code and the \texttt{falsifiability} Python library are available at \url{https://github.com/todd866/limits-of-falsifiability} and archived on Zenodo (DOI: \href{https://doi.org/10.5281/zenodo.17791874}{10.5281/zenodo.17791874}).

\begin{thebibliography}{99}

\bibitem{todd2025limits}
Todd, I. (2025). The limits of falsifiability: Dimensionality, measurement thresholds, and the sub-Landauer domain in biological systems. \textit{BioSystems}, 258, 105608. \url{https://doi.org/10.1016/j.biosystems.2025.105608}

\bibitem{lorenz1963}
Lorenz, E.N. (1963). Deterministic nonperiodic flow. \textit{Journal of the Atmospheric Sciences}, 20(2), 130--141.

\bibitem{stocks2000}
Stocks, N. (2000). Suprathreshold stochastic resonance in multilevel threshold systems. \textit{Physical Review Letters}, 84(11), 2310--2313.

\bibitem{mcdonnell2011}
McDonnell, M.D., Ward, L.M. (2011). The benefits of noise in neural systems: bridging theory and experiment. \textit{Nature Reviews Neuroscience}, 12(7), 415--426.

\bibitem{stringer2019}
Stringer, C., et al. (2019). Spontaneous behaviors drive multidimensional, brainwide activity. \textit{Science}, 364(6437), eaav7893.

\bibitem{donoho2000}
Donoho, D.L. (2000). High-dimensional data analysis: The curses and blessings of dimensionality. \textit{AMS Math Challenges Lecture}, 1--32.

\bibitem{gorban2018}
Gorban, A.N., Tyukin, I.Y. (2018). Blessing of dimensionality: mathematical foundations of the statistical physics of data. \textit{Philosophical Transactions of the Royal Society A}, 376(2118), 20170237.

\bibitem{azadi2025}
Azadi, S. (2025). Computational irreducibility as the foundation of agency. \textit{BioSystems}, 256, 105391.

\bibitem{louie2020}
Louie, A.H. (2020). Relational biology and Church's thesis. \textit{BioSystems}, 197, 104179.

\bibitem{igamberdiev2021}
Igamberdiev, A.U., Brenner, J.E. (2021). Mathematics in biological reality: The emergence of natural computation in living systems. \textit{BioSystems}, 204, 104395.

\end{thebibliography}

\end{document}
